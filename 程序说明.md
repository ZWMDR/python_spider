# 程序说明

### 1. 网络爬虫程序

#### 1.1 API接口

受限于豆瓣短评仅开放500条数据，我们选择爬取猫眼电影的影评。使用Chrome浏览器模拟手机登录猫眼电影，找出猫眼影评的数据接口，并通过多次尝试寻找URL规律，总结出如下爬取方案：

```
http://m.maoyan.com/mmdb/comments/movie/+电影ID号码+.json?_v_=yes&offset=20&startTime=+开始时间
```

URL第一、第三部分固定，第二部分为影片在猫眼数据库中的ID序号，最后一部分为检索时间，使用该URL每次能够得到到15条评论的JSON格式信息。通过如上方式多次迭代，从当前系统时间开始向前不断爬取，获取评论最后一条的时间作为下一次爬取的开始时间，直到评论时间超过电影上映时间为止。平均爬取速率（单线程）约为：25条/秒。

URL生成代码：

```
url = 'http://m.maoyan.com/mmdb/comments/movie/'+movie_ID+'.json?_v_=yes&offset=20&startTime=' + start_time.replace('  ', '%20')
```

#### 1.2 爬虫函数

##### 1.2.1 程序主循环

通过分析发现，爬虫程序爬取速率主要受到猫眼API接口的限制，因此，单线程爬取效率低下，考虑使用多线程并发爬取多部电影数据。程序主函数中使用threading库函数为每一部电影创建一个子进程，每个子进程独立运行，同时爬取。

```python
if __name__ == '__main__':
    # 电影上映时间
	movies=[("Marvel","248172",'2012-05-05  00:00:00')]
	
	#多线程并发爬取多部影评
	start_time=time.time()
	for i in range(len(movies)):
		t=threading.Thread(target=task,args=(movies[i],))
		t.start()
```

每个子进程循环爬取数据，保存到csv文件中，执行完成自动结束进程。主函数如下：

```python
def task(argv):
	movie=argv[0]
	movie_ID=argv[1]
	end_time=argv[2]
	start_time = datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S')
	
	while start_time > end_time:
        url = 'http://m.maoyan.com/mmdb/comments/movie/'+movie_ID+'.json?_v_=yes&offset=20&startTime=' + start_time.replace('  ', '%20')
        html = None
        print(url)
		#写入日志文件
		file=open(movie+"_stracp.log","a+")
		file.write(url)
		file.write("\r\n")#windows换行符
		
        try:
            html = requestApi(url)
        
        except Exception as e:#如果有异常,暂停一会再爬
            time.sleep(5)
            html = requestApi(url).encode('UTF-8')

        if html==None or html=='':
            start_time = datetime.datetime.strptime(start_time, '%Y-%m-%d  %H:%M:%S') + datetime.timedelta(seconds=-1)
            start_time = datetime.datetime.strftime(start_time, '%Y-%m-%d  %H:%M:%S')
            continue
        comments = getData(html)
        #print(url)
        start_time = comments[len(comments)-1][4] #获取每页中最后一条评论时间,每页有15条评论
        # print(start_time)
        
        #最后一条评论时间减一秒，避免爬取重复数据
        start_time = datetime.datetime.strptime(start_time, '%Y-%m-%d  %H:%M:%S') + datetime.timedelta(seconds=-1)
        start_time = datetime.datetime.strftime(start_time, '%Y-%m-%d  %H:%M:%S')
        saveData(comments,movie)
```

##### 1.2.1 获取JSON

爬虫使用requests库函数，添加headers和cookies，使用get()方法获取json数据，返回“UTF-8”重编码后的文本内容。

```python
try:
    r = requests.get(url, headers=headers)
	r.raise_for_status()
	return r.text
    
except requests.HTTPError as e:
	print(e) 
except requests.RequestException as e:
	print(e)
except:
	print("出错了")
```

##### 1.2.2 数据清洗

获取到json信息后，使用如下函数对获取到的json信息进行数据清洗，提取出我们需要的关键信息，用户账户、所在城市、评论内容、评分及评论时间，并存储为列表。

```python
def getData(html):    
    json_data = json.loads(html)['cmts']
    comments = []
    
    #解析数据并存入数组
    try:
        for item in json_data:
            comment = []
            comment.append(item['nickName'])
            comment.append(item['cityName'] if 'cityName' in item else '')
            comment.append(item['content'].strip().replace('\n', ' '))
            comment.append(item['score'])
            comment.append(item['startTime'])
            comments.append(comment)
        return comments
    
    except Exception as e:
        print(comment)
        print(e)
```

##### 1.2.3 保存数据

解析得到的数据转化为pandas.DataFrame追加写入，保存到csv文件中。

```python
def saveData(comments,movie):

    filename = './'+movie+'fulian_comments.csv'
    
    dataObject = pd.DataFrame(comments)
    dataObject.to_csv(filename, mode='a', index=False, sep=',', header=False,encoding='utf-8-sig')
```

#### 1.3 完整代码

```python
import requests,json,time,re,datetime
import pandas as pd
import threading

#请求评论api接口
def requestApi(url):  
    headers = {
        'accept': '*/*',
        'user-agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 11_0 like Mac OS X) AppleWebKit/604.1.38 (KHTML, like Gecko) Version/11.0 Mobile/15A372 Safari/604.1',
    }

    try:
        r = requests.get(url, headers=headers)
        r.raise_for_status()
        return r.text
    
    except requests.HTTPError as e:
        print(e) 
    except requests.RequestException as e:
        print(e)
    except:
        print("出错了")

#解析接口返回数据        
def getData(html):    
    json_data = json.loads(html)['cmts']
    comments = []
    #print("\n\n\njson_data:",json_data,"\n\n")
    #解析数据并存入数组
    try:
        for item in json_data:
            comment = []
            comment.append(item['nickName'])
            comment.append(item['cityName'] if 'cityName' in item else '')
            comment.append(item['content'].strip().replace('\n', ' '))
            comment.append(item['score'])
            comment.append(item['startTime'])
            comments.append(comment)
        return comments
    
    except Exception as e:
        print(comment)
        print(e)


#保存数据，写入excel        
def saveData(comments,movie):

    filename = './'+movie+'fulian_comments.csv'
    
    dataObject = pd.DataFrame(comments)
    dataObject.to_csv(filename, mode='a', index=False, sep=',', header=False,encoding='utf-8-sig')
	
      
def task(argv):
	movie=argv[0]
	movie_ID=argv[1]
	end_time=argv[2]
	start_time = datetime.datetime.now().strftime('%Y-%m-%d  %H:%M:%S')
	
	while start_time > end_time:
        url = 'http://m.maoyan.com/mmdb/comments/movie/'+movie_ID+'.json?_v_=yes&offset=20&startTime=' + start_time.replace('  ', '%20')
        html = None
        print(url)
		#写入日志文件
		file=open(movie+"_stracp.log","a+")
		file.write(url)
		file.write("\r\n")#windows换行符
		
        try:
            html = requestApi(url)
        
        except Exception as e:#如果有异常,暂停一会再爬
            time.sleep(5)
            html = requestApi(url).encode('UTF-8')

        if html==None or html=='':
            start_time = datetime.datetime.strptime(start_time, '%Y-%m-%d  %H:%M:%S') + datetime.timedelta(seconds=-1)
            start_time = datetime.datetime.strftime(start_time, '%Y-%m-%d  %H:%M:%S')
            continue
        comments = getData(html)
        #print(url)
        start_time = comments[14][4] #获取每页中最后一条评论时间,每页有15条评论
        # print(start_time)
        
        #最后一条评论时间减一秒，避免爬取重复数据
        start_time = datetime.datetime.strptime(start_time, '%Y-%m-%d  %H:%M:%S') + datetime.timedelta(seconds=-1)
        start_time = datetime.datetime.strftime(start_time, '%Y-%m-%d  %H:%M:%S')
        
        saveData(comments,movie)
	
if __name__ == '__main__':
    # 电影上映时间
	movies=[("Marvel","248172",'2012-05-05  00:00:00')]
	
	#多线程并发爬取多部影评
	start_time=time.time()
	for i in range(len(movies)):
		t=threading.Thread(target=task,args=(movies[i],))
		t.start()
```



### 2. 数据分析程序

#### 2.1 数据处理流程

数据分析的方面包括：

- 评论用户地理坐标分布

- 城市活跃用户排行榜

- 评论数量与日期分布

- 用户评论词云图

- 用户评论情感倾向分析

得益于python字典结构自带哈希散列，查找、统计效率极高，在不涉及大量计算时单线程处理也能表现出优异的性能。但在最后两个计算密集型部分，单线程运行效率过低，因此我们在此引入多进程并发处理多部影片数据的方式，对多部电影同时分析，提高CPU利用率。

```python
def analyze(argv):
    name=argv[0]
    path=argv[1]
    titles = ['nickName','cityName','content','score','startTime']
    comments=read_csv(path,titles)#加载csv文件
    
    draw_map(name,comments)#生成地理坐标图
    draw_bar(name,comments)#生成地理位置排行榜
    draw_DateBar(name,comments)#生成评论日期表
    draw_TimeBar(name,comments)#生成评论数量与时间关系图
    draw_wordCloud(name,comments)#生成词云图
    sentiments_analyze(name,comments)#情感倾向分析
    return
    
if __name__ == "__main__":
    movies={"《复联1》":"./fulian1_comments.csv","《复联2》":"./fulian2_comments.csv",
            "《复联3》":"./fulian3_comments.csv","《复联4》":"./fulian_comments.csv",
            "《奇异博士》":"strange.csv","《惊奇队长》":"marvel_comments.csv",
            "《黑豹》":"heibao_comments.csv","《银河护卫队1》":"galaxy1_comments.csv",
            "《银河护卫队2》":"galaxy2.csv","《蚁人1》":"yiren1_comments.csv",
            "《蚁人2》":"yiren2_comments.csv","《雷神1》":"thro1.csv",
            "《雷神2》":"thro2.csv","《雷神3》":"thro3.csv",
            "《钢铁侠1》":"Iron_man_comments.csv","《钢铁侠2》":"Iron2_comments.csv",
            "《钢铁侠3》":"Iron3_comments.csv","《美国队长1》":"captain1_comments.csv",
			"《美国队长2》":"captain2_comments.csv",
            "《美国队长3》":"captain3_comments.csv"}
    #多进程并发处理多部影片，提高CPU利用率
    multiprocessing.freeze_support()
    pool=multiprocessing.Pool(processes=multiprocessing.cpu_count())
    task=[]
    for name in movies.keys():
        task.append((name,movies[name]))
    
    #多进程并发分析数据，主进程阻塞
    start_time=time.time()
    pool.map(analyze,task)
    
    pool.close()
    pool.join()
    end_time=time.time()
    
    print("time=",end_time-start_time)#输出程序运行时间
    print("全部完成")
```

#### 2.2 数据加载

使用pandas库中read_csv()函数读取csv文件。

```python
def read_csv(filename, titles):
    comments = pd.read_csv(filename, names=titles, encoding='utf-8',lineterminator='\n')
    return comments
```

#### 2.3 数据可视化处理

##### 2.3.1 地域分布图

为了解评论用户地域分布情况，我们对用户城市数据进行分析，对出现频率最高的300个城市地理坐标绘制分布图，生成THML文件。

```python
def draw_map(movie,comments):
    try:
        attr = comments['cityName'].fillna("zero_token")
        data = Counter(attr).most_common(300)
        for item in data:
            if item[0]=="zero_token" or item[0]=="NaN" or item[0]=="":
                data.remove(item)
        geo = Geo(movie+"全国观众地域分布", "数据来源：猫眼电影", title_color="#fff", title_pos="center", width=1000, height=600, background_color='#404a59')
        attr, value = geo.cast(data)
        geo.add("", attr, value, visual_range=[0, 1000], maptype='china',visual_text_color="#fff", symbol_size=10, is_visualmap=True)
        geo.render("./"+movie+"观众地域分布-地理坐标图.html")
        geo
        print(movie+"全国观众地域分布已完成")
    except Exception as e:
        print(e)
```

##### 2.3.2 地理位置排行榜

对出现频率最高的20座城市进行统计，并绘制直方图。

```python
def draw_bar(movie,comments):
    data_top20 = Counter(comments['cityName']).most_common(20)
    bar = Bar(movie+'观众地域排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(data_top20)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'观众地域排行榜单-柱状图.html')
    print(movie+"观众地域排行榜单已完成")
```

##### 2.3.3 词云图

使用jieba.cut()函数进行自动截取，增加一些常见的停用词，以钢铁侠logo作为词云的形状，底色黑色，字体为隶书，最大字号200，制作词云图，并保存到"./"+movie+"WordCloud.jpg"。

```python
def draw_wordCloud(movie,comments):
    data = comments['content']

    comment_data = []
    print("开始分词，此处运行时间较长")
    for item in data:
        if pd.isnull(item) == False:
            comment_data.append(item)
 
    comment_after_split = jieba.cut(str(comment_data), cut_all=False)
    words = ' '.join(comment_after_split)
    
    #自定义停用词
    stopwords = STOPWORDS.copy()

    stopwords.add('一部')
    stopwords.add('一个')
    stopwords.add('没有')
    stopwords.add('什么')
    stopwords.add('有点')
    stopwords.add('不过')
    stopwords.add('但是')
    stopwords.add('还是')
    stopwords.add('感觉')  
    stopwords.add('就是')
    stopwords.add('觉得')
    stopwords.add('电影')
    stopwords.add('好看')
    stopwords.add('可以')
    stopwords.add('不错')

    alice_mask=np.array(Image.open("./背景2.jpg"))
    
    wc = WordCloud(width=1080, height=1920,
                   background_color='black',
                   font_path='C:\\Windows\\Fonts\\SIMLI.TTF',
                   scale=5, stopwords=stopwords,
                   max_font_size=200,random_state=50,
                   mask=alice_mask)
    wc=wc.generate(words)
    wc.to_file("./"+movie+"WordCloud.jpg")
 
    # plt.figure(figsize=(10, 8))
    plt.imshow(wc)
    plt.axis('off')
    plt.show()
```

##### 2.3.4 情感倾向分析

使用SnowNLP库函数对用户评论进行情感倾向分析。SnowNLP分析结果越接近1表示正面情感更丰富，支持率越高；越接近0则表明负面评论更多，支持率越低。对每一个人物的有关评论做Snow NLP分析，再将所得结果加权平均，得到支持率分布图。同时对用户评论中各个人物提及次数做出统计，得到观众关注率直方图。同时为了后续处理方便，将所得结果以JSON格式保存到“./影片名_semtiments.json”文件中。

```python
def sentiments_analyze(movie,comments):
    attr = ["灭霸","美国队长", "钢铁侠", "浩克", "奇异博士",  "蜘蛛侠", "索尔" ,
            "黑寡妇", "鹰眼", "惊奇队长", "幻视","猩红女巫","蚁人", "古一法师","星云"]
    alias = {"灭霸": ["灭霸", "Thanos","萨罗斯","灭世霸王"],
             "美国队长": ["美国队长", "美队","captain","Captain"],
             "浩克": ["浩克", "绿巨人", "班纳", "HULK","hulk"],
             "奇异博士": ["奇异博士", "医生","斯特兰奇","博士","strange"],
             "钢铁侠": ["钢铁侠", "stark", "斯塔克", "托尼", "史塔克","iron","Iron","铁人"],
             "蜘蛛侠": ["蜘蛛侠","蜘蛛","彼得", "荷兰弟"],
             "索尔":["索尔", "雷神"],
             "黑寡妇": ["黑寡妇", "寡姐"],
             "鹰眼":["鹰眼","克林顿","巴顿","克林特"],
             "惊奇队长":["惊奇队长","卡罗尔", "惊奇"],
             "幻视":["幻视"],
             "星云":["星云"],
             "猩红女巫": ["猩红女巫", "绯红女巫", "旺达"],
             "蚁人":["蚁人", "蚁侠", "Ant", "AntMan"],
             "古一法师": ["古一", "古一法师", "法师"]}
    sentiment={}
    SENTIMENT={}
    figure={}.fromkeys(attr,0)

    for att in attr:
        sentiment[att]=[0,0]
        SENTIMENT[att]=[]
    
    data = comments['content']
    data=np.array(data)
    print("data_len:",len(data))
    print("正在计算情感倾向，此处耗时较长")
    for i in range(len(data)):
        data[i]=str(data[i])
    
    start_time=time.time()
    
    #遍历每个人物的名称
    for item in data:
        judge={}.fromkeys(attr,False)
        for cha in attr:
            for figure_name in alias[cha]:
                items=re.findall(figure_name,item)
                length=len(items)
                figure[cha]+=length
                if length>0:
                    judge[cha]=True
    
        for cha in attr:
            if judge[cha]:
                s=SnowNLP(item).sentiments
                sentiment[cha][0]+=s
                sentiment[cha][1]+=1
                SENTIMENT[cha].append((s,sentiment[cha][1]))
    
    end_time=time.time()
    print("time=",end_time-start_time)
    count=sum(figure.values())
    for figure_name in figure:
        figure[figure_name]/=count
        if sentiment[figure_name][1]>0:
            sentiment[figure_name][0]/=sentiment[figure_name][1]

    print(movie,":",count,"\n",figure.items())
    print(movie,":\n",sentiment.items())

    bar = Bar(movie+'观众关注率排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(figure)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'人物关注率-柱状图.html')
    print(movie+"观众关注率排行榜单已完成")
    
    var={}
    for sent in sentiment.keys():
        var[sent]=sentiment[sent][0]
    bar = Bar(movie+'观众支持率排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(var)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'人物支持率-柱状图.html')
    print(movie+"观众支持率排行榜单已完成")
    file=open("./"+movie+"sentiments.json","w",encoding="utf-8")
    file.write(json.dumps(SENTIMENT,ensure_ascii=False))
    file.close()
```

#### 2.3 完整代码

```python
import pandas as pd
from collections import Counter
from pyecharts import Map, Geo, Bar
import jieba
import jieba.analyse
import matplotlib.pyplot as plt
from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator
from PIL import Image
import numpy as np
import re
from snownlp import SnowNLP
import json
import multiprocessing
import time


#读取csv文件数据 
def read_csv(filename, titles):
    comments = pd.read_csv(filename, names=titles, encoding='utf-8',lineterminator='\n')
    return comments

#观众地域图 
def draw_map(movie,comments):
    try:
        attr = comments['cityName'].fillna("zero_token")
        data = Counter(attr).most_common(300)
        for item in data:
            if item[0]=="zero_token" or item[0]=="NaN":
                data.remove(item)
        #data.remove(data[data.index([(i,x) for i,x in (data) if i == 'zero_token'][0])])
        geo = Geo(movie+"全国观众地域分布", "数据来源：猫眼电影", title_color="#fff", title_pos="center", width=1000, height=600, background_color='#404a59')
        attr, value = geo.cast(data)
        geo.add("", attr, value, visual_range=[0, 1000], maptype='china',visual_text_color="#fff", symbol_size=10, is_visualmap=True)
        geo.render("./"+movie+"观众地域分布-地理坐标图.html")
        geo
        print(movie+"全国观众地域分布已完成")
    except Exception as e:
        print(e)

def draw_bar(movie,comments):
    data_top20 = Counter(comments['cityName']).most_common(20)
    bar = Bar(movie+'观众地域排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(data_top20)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'观众地域排行榜单-柱状图.html')
    print(movie+"观众地域排行榜单已完成")
    data_top20=np.array(data_top20)
    file=open("Top20.data","w",encoding="UTF-8")
    for item in data_top20:
        file.write("(")
        file.write(item[0])
        file.write(",")
        file.write(item[1])
        file.write(")")
        file.write("\r\n")
    file.close()
    
def draw_wordCloud(movie,comments):
    data = comments['content']

    comment_data = []
    print("开始分词，此处运行时间较长")
    for item in data:
        if pd.isnull(item) == False:
            comment_data.append(item)
 
    comment_after_split = jieba.cut(str(comment_data), cut_all=False)
    words = ' '.join(comment_after_split)
    
    #自定义停用词
    stopwords = STOPWORDS.copy()

    stopwords.add('一部')
    stopwords.add('一个')
    stopwords.add('没有')
    stopwords.add('什么')
    stopwords.add('有点')
    stopwords.add('不过')
    stopwords.add('但是')
    stopwords.add('还是')
    stopwords.add('感觉')  
    stopwords.add('就是')
    stopwords.add('觉得')
    stopwords.add('电影')
    stopwords.add('好看')
    stopwords.add('可以')
    stopwords.add('不错')

    alice_mask=np.array(Image.open("./背景2.jpg"))
    
    wc = WordCloud(width=1080, height=1920,
                   background_color='black',
                   font_path='C:\\Windows\\Fonts\\SIMLI.TTF',
                   scale=5, stopwords=stopwords,
                   max_font_size=200,random_state=50,
                   mask=alice_mask)
    wc=wc.generate(words)
    wc.to_file("./"+movie+"WordCloud.jpg")
 
    # plt.figure(figsize=(10, 8))
    plt.imshow(wc)
    plt.axis('off')
    plt.show()
 
def draw_DateBar(movie,comments):
    time = comments['startTime']
    timeData = []
    for t in time:
        if pd.isnull(t) == False:
            date = t.split(' ')[0]
            timeData.append(date)

    data = Counter(timeData).most_common()
    data = sorted(data, key=lambda data : data[0]) 
   
    
    bar = Bar(movie+'观众评论数量与日期的关系', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(data)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 3500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'观众评论日期-柱状图.html')
    print(movie+"观众评论数量与日期的关系已完成")
  

def draw_TimeBar(movie,comments):
    time = comments['startTime']
    timeData = []
    for t in time:
        if pd.isnull(t) == False:
            time = t.split(' ')[1]
            hour = time.split(':')[0]
            timeData.append(hour)
 
    data = Counter(timeData).most_common()
    data = sorted(data, key=lambda data : data[0])    
    
    bar = Bar(movie+'观众评论数量与时间的关系', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(data)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 3500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'观众评论时间-柱状图.html')
    print(movie+"观众评论数量与时间的关系已完成")


def sentiments_analyze(movie,comments):
    attr = ["灭霸","美国队长", "钢铁侠", "浩克", "奇异博士",  "蜘蛛侠", "索尔" ,
            "黑寡妇", "鹰眼", "惊奇队长", "幻视","猩红女巫","蚁人", "古一法师","星云"]
    alias = {"灭霸": ["灭霸", "Thanos","萨罗斯","灭世霸王"],
             "美国队长": ["美国队长", "美队","captain","Captain"],
             "浩克": ["浩克", "绿巨人", "班纳", "HULK","hulk"],
             "奇异博士": ["奇异博士", "医生","斯特兰奇","博士","strange"],
             "钢铁侠": ["钢铁侠", "stark", "斯塔克", "托尼", "史塔克","iron","Iron","铁人"],
             "蜘蛛侠": ["蜘蛛侠","蜘蛛","彼得", "荷兰弟"],
             "索尔":["索尔", "雷神"],
             "黑寡妇": ["黑寡妇", "寡姐"],
             "鹰眼":["鹰眼","克林顿","巴顿","克林特"],
             "惊奇队长":["惊奇队长","卡罗尔", "惊奇"],
             "幻视":["幻视"],
             "星云":["星云"],
             "猩红女巫": ["猩红女巫", "绯红女巫", "旺达"],
             "蚁人":["蚁人", "蚁侠", "Ant", "AntMan"],
             "古一法师": ["古一", "古一法师", "法师"]}
    sentiment={}
    SENTIMENT={}
    figure={}.fromkeys(attr,0)

    for att in attr:
        sentiment[att]=[0,0]
        SENTIMENT[att]=[]
    
    data = comments['content']
    data=np.array(data)
    print("data_len:",len(data))
    print("正在计算情感倾向，此处耗时较长")
    for i in range(len(data)):
        data[i]=str(data[i])
    
    start_time=time.time()
    
    #遍历每个人物的名称
    for item in data:
        judge={}.fromkeys(attr,False)
        for cha in attr:
            for figure_name in alias[cha]:
                items=re.findall(figure_name,item)
                length=len(items)
                figure[cha]+=length
                if length>0:
                    judge[cha]=True
    
        for cha in attr:
            if judge[cha]:
                s=SnowNLP(item).sentiments
                sentiment[cha][0]+=s
                sentiment[cha][1]+=1
                SENTIMENT[cha].append((s,sentiment[cha][1]))
    
    end_time=time.time()
    print("time=",end_time-start_time)
    count=sum(figure.values())
    for figure_name in figure:
        figure[figure_name]/=count
        if sentiment[figure_name][1]>0:
            sentiment[figure_name][0]/=sentiment[figure_name][1]

    print(movie,":",count,"\n",figure.items())
    print(movie,":\n",sentiment.items())

    bar = Bar(movie+'观众关注率排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(figure)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'人物关注率-柱状图.html')
    print(movie+"观众关注率排行榜单已完成")
    
    var={}
    for sent in sentiment.keys():
        var[sent]=sentiment[sent][0]
    bar = Bar(movie+'观众支持率排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600)
    attr, value = bar.cast(var)
    bar.add('', attr, value, is_visualmap=True, visual_range=[0, 4500], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./'+movie+'人物支持率-柱状图.html')
    print(movie+"观众支持率排行榜单已完成")
    file=open("./"+movie+"sentiments.json","w",encoding="utf-8")
    file.write(json.dumps(SENTIMENT,ensure_ascii=False))
    file.close()
    
def analyze(argv):
    name=argv[0]
    path=argv[1]
    titles = ['nickName','cityName','content','score','startTime']
    comments=read_csv(path,titles)#加载csv文件
    
    draw_map(name,comments)#生成地理坐标图
    draw_bar(name,comments)#生成地理位置排行榜
    draw_DateBar(name,comments)#生成评论日期表
    draw_TimeBar(name,comments)#生成评论数量与时间关系图
    draw_wordCloud(name,comments)#生成词云图
    sentiments_analyze(name,comments)#情感倾向分析
    return
    
if __name__ == "__main__":
    movies={"《复联1》":"./fulian1_comments.csv","《复联2》":"./fulian2_comments.csv",
            "《复联3》":"./fulian3_comments.csv","《复联4》":"./fulian_comments.csv",
            "《奇异博士》":"strange.csv","《惊奇队长》":"marvel_comments.csv",
            "《黑豹》":"heibao_comments.csv","《银河护卫队1》":"galaxy1_comments.csv",
            "《银河护卫队2》":"galaxy2.csv","《蚁人1》":"yiren1_comments.csv",
            "《蚁人2》":"yiren2_comments.csv","《雷神1》":"thro1.csv",
            "《雷神2》":"thro2.csv","《雷神3》":"thro3.csv",
            "《钢铁侠1》":"Iron_man_comments.csv","《钢铁侠2》":"Iron2_comments.csv",
            "《钢铁侠3》":"Iron3_comments.csv","《美国队长1》":"captain1_comments.csv",
			"《美国队长2》":"captain2_comments.csv",
            "《美国队长3》":"captain3_comments.csv"
			}
    #多进程并发处理多部影片，提高CPU利用率
    multiprocessing.freeze_support()
    pool=multiprocessing.Pool(processes=multiprocessing.cpu_count())
    task=[]
    for name in movies.keys():
        task.append((name,movies[name]))
    
    #多进程并发分析数据，主进程阻塞
    start_time=time.time()
    pool.map(analyze,task)
    
    pool.close()
    pool.join()

    end_time=time.time()
    
    print("time=",end_time-start_time)#输出程序运行时间
    print("全部完成")
```

### 3. 数据整合

对每一部影片分析得到的数据，我们对复联中主要人物在每一部影片中的结果绘制横向对比图，显示每一个人物在每一部影片中的受欢迎程度。最后，通过自定义算法，通过计算得到每一位人物的综合评分，寻找出“最佳C位”。

#### 3.1 横向对比

循环读取每一部影片在初步分析部分保存得到的JSON文件，提取并保存每一位人物的所有相关影评的SnowNLP评分结果，并对每一位人物在每一部影片中的SnowNLP评分结果数学期望、影评关注率和SnowNLP结果的离散程度（方差）分别绘制图表。

```python
def horizental_analyze(movies,attr):
    counts={}
    dispers={}
    sentiments={}
    for movie in movies:
        path=movie+"sentiments.json"
        file=open(path,"r",encoding='utf-8')
        sentiment=json.loads(file.read())
        file.close()
        st={}.fromkeys(attr,0)
        count={}.fromkeys(attr,0)
        disper={}.fromkeys(attr,0)
        for figure in attr:
            count[figure]=len(sentiment[figure])
            for item in sentiment[figure]:
                st[figure]+=item[0]
            if len(sentiment[figure])>0:
                st[figure]=round(st[figure]/len(sentiment[figure]),4)
        sentiments[movie]=st
        counts[movie]=count
        
        for figure in attr:
            for item in sentiment[figure]:
                disper[figure]+=(item[0]-st[figure])**2
        dispers[movie]=disper
    
    sum_counts={}.fromkeys(movies,0)
    for figure in attr:
        for movie in movies:
            sum_counts[movie]+=counts[movie][figure]
    
    figures_NLP={}
    figures_con={}
    figures_disp={}
    for figure in attr:
        NLP={}.fromkeys(movies,0)
        con={}.fromkeys(movies,0)
        disp={}.fromkeys(movies,0)
        for movie in movies:
            NLP[movie]=sentiments[movie][figure]
            con[movie]=round(counts[movie][figure]/sum_counts[movie],4)
            disp[movie]=round(math.sqrt(dispers[movie][figure]/sum_counts[movie]),4)
        figures_NLP[figure]=NLP
        figures_con[figure]=con
        figures_disp[figure]=disp   
   
    re_attr=["灭霸","美国队长", "钢铁侠", "绿巨人", "奇异博士",  "蜘蛛侠", "雷神",
            "黑寡妇", "鹰眼", "惊奇队长", "幻视","猩红女巫","蚁人", "古一法师","星云"]
    
    re_movies=["《复仇者联盟》","《复仇者联盟2》","《复仇者联盟3：\n无限战争》","《复仇者联盟4：\n终局之战》",
            "《奇异博士》","《惊奇队长》","《黑豹》","《银河护卫队1》","《银河护卫队2》",
            "《蚁人1》","《蚁人2》","《雷神1》","《雷神2》","《雷神3》","《钢铁侠1》",
            "《钢铁侠2》","《钢铁侠3》","《美国队长1》","《美国队长2》","《美国队长3》"]
    for i in range(len(attr)):
        bar=Bar("\""+re_attr[i]+'\"受欢迎程度横比', '数据来源：猫眼电影',title_pos='left',
                title_top='top',width=1200,height=560,background_color='white')
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_NLP[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众感情倾向变化趋势",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_disp[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众感情倾向离散度",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_con[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众关注度",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        bar.render('./'+re_attr[i]+'受欢迎程度横比-柱状图.html')
        
    print("人物支持率变化趋势表已完成")
```

#### 3.2 综合排名

统计每部电影中，每位复联人物的SnowNLP期望、均方差和关注率的综合得分，按照每一项权重$0.75$、$0.15$、$0.1$的方式，按照如下公式计算得到最终得分：（$X_i$表示每一位人物的得分集合，$Y_i$表示每一位人物的评论次数）
$$
\begin{split}
FinalScore_i=&0.75\times E(X_{i})_{SnowNLP} \\
+&0.15\times \frac{Y_i}{\sum{Y_i}}\\
+&0.10\times \frac{\left(1-\sigma(X_{i})_{SnowNLP} \right)}{max\left\{ \sigma(X_{i})_{SnowNLP}\right\} }
\end{split}
$$
将得到的结果排名后，绘制最终的直方图。

```python
def reanalyze(data,attr):
    re_data={}.fromkeys(attr,0)
    re_data_NLP={}.fromkeys(attr,0)
    re_data_disp={}.fromkeys(attr,0)
    re_data_c={}.fromkeys(attr,0)
    count=0
    for figure in attr:
        score=0
        amount=len(data[figure])
        count+=amount
        for item in data[figure]:
            score+=item[0]
        re_data_NLP[figure]=round(score/amount,4)
        re_data[figure]=amount
    for figure in attr:
        re_data[figure]/=count
        re_data[figure]=round(re_data[figure],4)
    
    for figure in attr:
        score=0
        for item in data[figure]:
            score+=(re_data_NLP[figure]-item[0])**2
        score/=count
        score=math.sqrt(score)
        re_data_disp[figure]=round(score,4)
    
    max_conc=max([score for score in re_data.values()])
    max_NLP=max([score for score in re_data_NLP.values()])
    max_disp=max([score for score in re_data_disp.values()])
    for figure in attr:
        re_data_c[figure]=round(0.75*re_data_NLP[figure]/max_NLP 
                               + 0.15*re_data[figure]/max_conc
                               + 0.1*(max_disp-re_data_disp[figure])/max_disp,4)
    
    sorted_data=sorted(re_data.items(),key=lambda x:x[1],reverse=True)
    sorted_data_NLP=sorted(re_data_NLP.items(),key=lambda x:x[1],reverse=True)
    sorted_data_disp=sorted(re_data_disp.items(),key=lambda x:x[1],reverse=True)
    sorted_data_c=sorted(re_data_c.items(),key=lambda x:x[1],reverse=True)
    
    return sorted_data,sorted_data_NLP,sorted_data_disp,sorted_data_c
    
def multi_analyze(data,attr):
    data,data_NLP,data_disp,data_c=reanalyze(data,attr)
    
    bar=Bar('观众支持率综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_NLP)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物支持率-柱状图.html')
    print("综合观众支持率排行榜单已完成")
    
    bar=Bar('观众关注率综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物关注率-柱状图.html')
    print("综合观众关注率排行榜单已完成")
    
    bar=Bar('观众情感倾向离散度综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_disp)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物情感倾向离散度-柱状图.html')
    print("综合观众情感倾向离散度排行榜单已完成")
    
    bar=Bar('谁是观众心目中的最强C位？', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_c)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./最强C位-柱状图.html')
    print("最强C位综合排行榜单已完成")
```

#### 3.3 完整代码

```python
# -*- coding: utf-8 -*-
"""
Created on Sat Jun  1 15:53:03 2019

@author: ZWMDR
"""

import json
from pyecharts import Bar,Line,EffectScatter
import math
import numpy as np

def load_jsons(movies):
    sentiments=[]
    for movie in movies:
        path=movie+"sentiments.json"
        file=open(path,"r",encoding='utf-8')
        sentiment=json.loads(file.read())
        file.close()
        sentiments.append(sentiment)
    return sentiments

def integrate_dict(sentiments,attr):
    data={}
    for figure in attr:
        data[figure]=[]
    for sentiment in sentiments:
        for figure in attr:
            for sent in sentiment[figure]:
                data[figure].append(sent)
    return data

def reanalyze(data,attr):
    re_data={}.fromkeys(attr,0)
    re_data_NLP={}.fromkeys(attr,0)
    re_data_disp={}.fromkeys(attr,0)
    re_data_c={}.fromkeys(attr,0)
    count=0
    for figure in attr:
        score=0
        amount=len(data[figure])
        count+=amount
        for item in data[figure]:
            score+=item[0]
        re_data_NLP[figure]=round(score/amount,4)
        re_data[figure]=amount
    for figure in attr:
        re_data[figure]/=count
        re_data[figure]=round(re_data[figure],4)
    
    for figure in attr:
        score=0
        for item in data[figure]:
            score+=(re_data_NLP[figure]-item[0])**2
        score/=count
        score=math.sqrt(score)
        re_data_disp[figure]=round(score,4)
    
    max_conc=max([score for score in re_data.values()])
    max_NLP=max([score for score in re_data_NLP.values()])
    max_disp=max([score for score in re_data_disp.values()])
    for figure in attr:
        re_data_c[figure]=round(0.75*re_data_NLP[figure]/max_NLP 
                               + 0.15*re_data[figure]/max_conc
                               + 0.1*(max_disp-re_data_disp[figure])/max_disp,4)
    
    sorted_data=sorted(re_data.items(),key=lambda x:x[1],reverse=True)
    sorted_data_NLP=sorted(re_data_NLP.items(),key=lambda x:x[1],reverse=True)
    sorted_data_disp=sorted(re_data_disp.items(),key=lambda x:x[1],reverse=True)
    sorted_data_c=sorted(re_data_c.items(),key=lambda x:x[1],reverse=True)
    
    return sorted_data,sorted_data_NLP,sorted_data_disp,sorted_data_c
    
def multi_analyze(data,attr):
    data,data_NLP,data_disp,data_c=reanalyze(data,attr)
    
    bar=Bar('观众支持率综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_NLP)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物支持率-柱状图.html')
    print("综合观众支持率排行榜单已完成")
    
    bar=Bar('观众关注率综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物关注率-柱状图.html')
    print("综合观众关注率排行榜单已完成")
    
    bar=Bar('观众情感倾向离散度综合排行榜单', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_disp)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./综合人物情感倾向离散度-柱状图.html')
    print("综合观众情感倾向离散度排行榜单已完成")
    
    bar=Bar('谁是观众心目中的最强C位？', '数据来源：猫眼电影', title_pos='center', width=1200, height=600,background_color='white')
    attr,value=bar.cast(data_c)
    upper=max(value)
    floor=min(value)
    bar.add('', attr, value, is_visualmap=True, visual_range=[floor,upper], visual_text_color='#fff', is_more_utils=True, is_label_show=True)
    bar.render('./最强C位-柱状图.html')
    print("最强C位综合排行榜单已完成")

def horizental_analyze(movies,attr):
    counts={}
    dispers={}
    sentiments={}
    for movie in movies:
        path=movie+"sentiments.json"
        file=open(path,"r",encoding='utf-8')
        sentiment=json.loads(file.read())
        file.close()
        st={}.fromkeys(attr,0)
        count={}.fromkeys(attr,0)
        disper={}.fromkeys(attr,0)
        for figure in attr:
            count[figure]=len(sentiment[figure])
            for item in sentiment[figure]:
                st[figure]+=item[0]
            if len(sentiment[figure])>0:
                st[figure]=round(st[figure]/len(sentiment[figure]),4)
        sentiments[movie]=st
        counts[movie]=count
        
        for figure in attr:
            for item in sentiment[figure]:
                disper[figure]+=(item[0]-st[figure])**2
        dispers[movie]=disper
    
    sum_counts={}.fromkeys(movies,0)
    for figure in attr:
        for movie in movies:
            sum_counts[movie]+=counts[movie][figure]
    
    figures_NLP={}
    figures_con={}
    figures_disp={}
    for figure in attr:
        NLP={}.fromkeys(movies,0)
        con={}.fromkeys(movies,0)
        disp={}.fromkeys(movies,0)
        for movie in movies:
            NLP[movie]=sentiments[movie][figure]
            con[movie]=round(counts[movie][figure]/sum_counts[movie],4)
            disp[movie]=round(math.sqrt(dispers[movie][figure]/sum_counts[movie]),4)
        figures_NLP[figure]=NLP
        figures_con[figure]=con
        figures_disp[figure]=disp
    
    """
    for figure in attr:
        bar=Bar("\""+figure+'\"观众感情倾向变化趋势', '数据来源：猫眼电影',title_pos='center',
                title_top='top',width=1200,height=560,background_color='white')
        
        items=[]
        values=[]
        for movie in movies:
            items.append(movie)
            values.append(figures_NLP[figure][movie])
        
        upper=max(values)
        floor=min(values)
        bar.add("",items,values, is_visualmap=True,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=10,
                xaxis_type="category")

        bar.render('./'+figure+'人物支持率变化趋势-柱状图.html')
        
        bar=Bar("\""+figure+'\"观众关注度变化趋势', '数据来源：猫眼电影',title_pos='center',
                title_top='top',width=1200,height=560,background_color='white')
        items=[]
        values=[]
        for movie in movies:
            items.append(movie)
            values.append(figures_con[figure][movie])
        
        upper=max(values)
        floor=min(values)
        bar.add("",items,values, is_visualmap=True,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=10,
                xaxis_type="category")

        bar.render('./'+figure+'人物关注度变化趋势-柱状图.html')
        
        
        bar=Bar("\""+figure+'\"观众情感倾向离散度变化趋势', '数据来源：猫眼电影',title_pos='center',
                title_top='top',width=1200,height=560,background_color='white')
        
        items=[]
        values=[]
        for movie in movies:
            items.append(movie)
            values.append(figures_disp[figure][movie])
        
        upper=max(values)
        floor=min(values)
        bar.add("",items,values, is_visualmap=True,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=10,
                xaxis_type="category")

        bar.render('./'+figure+'人物情感倾向离散度变化趋势-柱状图.html')
    """
    
    
    re_attr=["灭霸","美国队长", "钢铁侠", "绿巨人", "奇异博士",  "蜘蛛侠", "雷神",
            "黑寡妇", "鹰眼", "惊奇队长", "幻视","猩红女巫","蚁人", "古一法师","星云"]
    
    re_movies=["《复仇者联盟》","《复仇者联盟2》","《复仇者联盟3：\n无限战争》","《复仇者联盟4：\n终局之战》",
            "《奇异博士》","《惊奇队长》","《黑豹》","《银河护卫队1》","《银河护卫队2》",
            "《蚁人1》","《蚁人2》","《雷神1》","《雷神2》","《雷神3》","《钢铁侠1》",
            "《钢铁侠2》","《钢铁侠3》","《美国队长1》","《美国队长2》","《美国队长3》"]
    for i in range(len(attr)):
        bar=Bar("\""+re_attr[i]+'\"受欢迎程度横比', '数据来源：猫眼电影',title_pos='left',
                title_top='top',width=1200,height=560,background_color='white')
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_NLP[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众感情倾向变化趋势",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_disp[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众感情倾向离散度",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        
        items=[]
        values=[]
        for k in range(len(movies)):
            items.append(re_movies[k])
            values.append(figures_con[attr[i]][movies[k]])
        
        upper=max(values)
        floor=min(values)
        bar.add("观众关注度",items,values, is_visualmap=False,visual_range=[floor,upper],
                xaxis_rotate=45,visual_text_color='#fff',is_more_utils=True, 
                is_label_show=True,xaxis_margin=5,xaxis_label_textsize=11,
                xaxis_type="category")
        bar.render('./'+re_attr[i]+'受欢迎程度横比-柱状图.html')
        
    print("人物支持率变化趋势表已完成")
    
if __name__=="__main__":
    movies=["《复仇者联盟》","《复仇者联盟2》","《复仇者联盟3：无限战争》","《复仇者联盟4：终局之战》",
            "《奇异博士》","《惊奇队长》","《黑豹》","《银河护卫队1》","《银河护卫队2》",
            "《蚁人1》","《蚁人2》","《雷神1》","《雷神2》","《雷神3》","《钢铁侠1》",
            "《钢铁侠2》","《钢铁侠3》","《美国队长1》","《美国队长2》","《美国队长3》"]
    
    attr = ["灭霸","美国队长", "钢铁侠", "浩克", "奇异博士",  "蜘蛛侠", "索尔",
            "黑寡妇", "鹰眼", "惊奇队长", "幻视","猩红女巫","蚁人", "古一法师","星云"]
    
    sentiments=load_jsons(movies)
    data=integrate_dict(sentiments,attr)
    #multi_analyze(data,attr)
    movies=["《复仇者联盟》","《钢铁侠3》","《银河护卫队1》","《美国队长2》","《复仇者联盟2》",
            "《蚁人1》","《美国队长3》","《奇异博士》","《银河护卫队2》","《黑豹》","《复仇者联盟3：无限战争》",
            "《惊奇队长》","《蚁人2》","《复仇者联盟4：终局之战》"]
    
    horizental_analyze(movies,attr)
```



